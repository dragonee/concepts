# 21 konceptów dla liderów 21 wieku

1. System uporządkowany
   - Część frameworku Cynefin ([wiki](https://en.wikipedia.org/wiki/Cynefin_framework), [website](https://cynefin.io/wiki/Main_Page))
   - Przykład: komputer
   - Przykład: procedura krojenia nożem
   - Cecha: przewidywalne i powtarzalne działanie
   - Cecha: potrafi zepsuć się katastrofalnie w nieprzewidzianych przez twórcę warunkach 
     - komputer może się spalić w wyniku zakurzenia
     - nożem możemy się skaleczyć, bo np. został on naostrzony bez naszej wiedzy i kroi dwa razy efektywniej
2. System chaotyczny
   - Część frameworku Cynefin ([wiki](https://en.wikipedia.org/wiki/Cynefin_framework), [website](https://cynefin.io/wiki/Main_Page))
   - Przykład: modele przewidywania pogody (weather forecasting models)
   - Przykład: [problem trzech ciał](https://en.wikipedia.org/wiki/Three-body_problem)
   - Cecha: nieprzewidywalne do tego stopnia, że nawet nie da się wychwycić związków przyczynowo skutkowych. Rzeczy po prostu się dzieją.
3. System złożony adaptacyjny (en. complex adatptive system)
   - Część frameworku Cynefin ([wiki](https://en.wikipedia.org/wiki/Cynefin_framework), [website](https://cynefin.io/wiki/Main_Page))
   - Przykład: dowolna grupa ludzi, ich zachowania
   - Przykład: ekosystem parku ([wilki w Yellowstone](https://www.youtube.com/watch?v=Fiu8E0OR5Tg))
   - Cecha: nie da się przewidzieć przyszłości
   - Cecha: da się zauważyć związki przyczynowo-skutkowe w przeszłości (nie wszystkie)
   - Cecha: powtarzanie tego, co się robiło w przeszłości niekoniecznie będzie dawało te same efekty w przyszłości
4. Antykruchość
   - [Nassim Taleb – Antifragile](https://en.wikipedia.org/wiki/Antifragile_(book))
   - Cecha systemów, które pod wpływem stresorów, szoków, stają się mocniejsze
     - w przeciwieństwie do odporności (resilience), która polega na wytrzymaniu wpływu stresora
   - Przykład: Układ kostny pod wpływem złamań czasami staje się mocniejszy w miejscu złamania
5. Metoda sztangi
   - Z tej samej książki, co (4), atrybuowana Senece
   - Prosta strategia inwestycyjna, w której z jednej strony zapewniamy sobie bezpieczeństwo (finansowe, zatrudnienia, itp.) a pozostałe nam zasoby przeznaczamy w inwestycje o większym ryzyku, przy założeniu, że nie boli nas utrata tych pieniędzy, a zysk wynagrodzi nam wydane przez nas zasoby
   - Przykład: pracujemy na niskopłatnym stanowisku, a wolne pieniądze przeznaczamy na bycie na konferencjach i szukanie lepszej pracy.
   - Przykłady [tutaj](https://nowyobywatel.pl/2016/10/05/antykruche-panstwo-dobrobytu/)
6. Aktant
   - Pojęcie z [Actor-Network Theory](https://en.wikipedia.org/wiki/Actor%E2%80%93network_theory) Bruno Latoura, potem reużyte przez Dave'a Snowdena
   - W kontekście systemów złożonych adaptacyjnych oznacza każdy ludzi lub nieludzki czynnik, który ma wpływ i pewną sprawczość w danym systemie
   - W odróżnieniu od pojęcia aktora, który przypisuje sprawczość jedynie ludziom, aktantem może być dowolna grupa ludzi, plotka, dostępna wiedza, ograniczenia nałożone na system (celowo bądź przypadkowo), obecne trendy, itp.
   - Zobacz [pojęcie gry](gra.md) jako dalszą dyskusję i obszerniejszy przykład.
7. Afordancja - [Affordance](https://en.wikipedia.org/wiki/Affordance)
   - Najprościej opisując – lista możliwości, które dane środowisko lub obiekt nam daje
   - Osoby, które "zawsze dostrzegają okazję", "mają smykałkę do biznesu" często właśnie są specjalistami w wykrywaniu nieoczywistych afordancji, które daje im środowisko 
   - W zespołach możemy tworzyć nowe afordancje, które sprzyjać będą naszym celom
   - Przykład: możemy stworzyć program, w którym nagradzamy uznaniem i pieniędzmi za wymyślenie usprawnienia, które sprawi, że praca w fabryce będzie bezpieczniejsza lub sprawniejsza
8. Rekontekstualizacja ([wiki](https://en.wikipedia.org/wiki/Recontextualisation))
   - Termin wywodzi się z literaturoznawstwa
   - Oznacza przedstawienie czegoś w nowym kontekście
   - Przykład: internetowe memy; soundbite'y polityków
9. Mechanizmy radzenia sobie - [Coping mechanisms](https://en.wikipedia.org/wiki/Coping)
   - Termin z psychologii, oznacza nasze strategie radzenia sobie bólem, dyskomfortem psychicznym, stresem, presją itd.
   - Mogą być zdrowe i niezdrowe
     - Zdrowe dążą do rozwiązania problemu, który leży u źródła
     - Niezdrowe mają na celu usunięcie emocji
   - Automatyzmy w tej strefie to [mechanizmy obronne](https://pl.wikipedia.org/wiki/Mechanizmy_obronne) / [defense mechanisms](https://en.wikipedia.org/wiki/Defence_mechanism)
10. Przeramowanie kognitywne
    - Jeden z mechanizmów radzenia sobie (9)
    - Za pomocą odpowiednio dopasowanej rekontekstualizacji, przedstawiamy nasze wspomnienia w świetle dla nas korzystnym
    - Przykład: Dwie osoby w związku. Jedna cały czas mówi, że bez drugiej sobie nie poradzi, druga jest tym przytłoczona i czuje się winna, że nie nie daje wystarczająco wiele. Przeramowanie: druga osoba zmienia swoje myślenie na "daję tyle ile mogę, żeby nie czuć przytłoczenia, jeśli to za mało, to musimy się rozstać".
11. Myślenie abdukcyjne
    - Jeden z rodzajów myślenia
    - W odróżnieniu od indukcyjnego (na podstawie wielu przypadków zakładamy, że kolejny też taki będzie) i dedukcyjnego (na podstawie rachunku zdań logicznych, np. każdy człowiek to zwierzę, a więc jeśli jestem człowiekiem to jestem zwierzęciem)...
    - ... Myślenie abdukcyjne polega na znalezieniu najprostszego pasującego wydarzenia do zestawu znanych faktów i wyciąga wiarygodne, ale niepotwierdzone wytłumaczenia tych faktów.
    - Uwaga, jeśli zestawimy ze sobą kilka losowych rzeczy, możemy dojść do dobrze brzmiących, ale fałszych wytłumaczeń.
    - Posługujemy się nim na codzień.
    - Może powodować konflikty, bo lubimy sobie dopowiadać różnego rodzaju brakujące nam do wytłumaczenia fakty.
      - Wszelkie wariacje "Przecież to powiedziałeś! / Myślałem coś innego" są wynikiem rozumowania abduktywnego u dwóch osób, które dało dwa podobne, ale inne wytłumaczenia.
12. Halucynacje (AI)
    - W AI istnieje koncept halucynacji.
    - AI trenowane jest na pewnym zbiorze danych i nie posiada wiedzy na zewnątrz tego zbioru, nie może więc zwalidować swoich odpowiedzi.
    - Przy zadanym pytaniu, AI korzysta z myślenia abduktywnego i dopowiada sobie pasujące mu fakty.
    - To samo robimy my, tylko mamy większe zasoby intelektualne.
    - Wniosek: my też halucyjnujemy, nawet jeśli wydaje nam się, że jesteśmy najbardziej racjonalnymi osobami na świecie.
13. Fake guru
    - W świecie trenerów i innych osób związanych z rozwojem osobistym, wiele z nich wciska ludziom kit.
    - Niektóre z nich mogą nie być tego świadome (przez co wydają się bardzo autentyczne), niektóre są.
    - Artur Król: [Fake Guru](https://blog.krolartur.com/cykle/fake-guru/), [Anty-guru](https://blog.krolartur.com/cykle/anty-guru/)
    - Polecam też [Organized Collection of Irrational Nonsense](https://ozgurnevres.com/organized-collection-irrational-nonsense/)
14. Hiperpolaryzacja świata
    - Część [mapy trendów infuture.institute](https://infuture.institute/mapa-trendow/) - tam opisane jako po prostu Polaryzacja
    - Przyczyna: dostęp do mediów i tani koszt produkcji materiałów audio/video i tekstu
    - Każdy może produkować treści
    - Każdy może wybierać, co chce oglądać
    - Bańki informacyjne - oglądamy to, co potwierdza nasze myślenie
    - Efekt: dwie osoby w pokoju mogą totalnie różnić się bieżącą oceną swojego otoczenia, ponieważ nigdy nie oglądały tych materiałów, które oglądała druga osoba
15. Sygnałowanie - [Virtue signalling](https://en.wikipedia.org/wiki/Virtue_signalling)
    - Mówimy o czymś tylko po to, aby w oczach publiki przedstawić się w określony sposób
    - Nie musimy popierać działaniem naszych słów, więc mówić możemy wszystko
    - Jest to możliwe w miejscach, gdzie ciężko jest sprawdzić, co kto faktycznie robi (np. Internet)
    - Przykład: premier w mediach mówi, że dana ustawa jest bardzo ważna, po czym odsuwa ją w nieskońćzoność
    - Przykład: opowiadamy się za jakąś stroną w popularnym gorącym temacie debat tylko po to, żeby o nas też mówiono
16. Amplifikacja
    - Wzmacnianie komunikacji tak, żeby wyglądała na bardziej rozpowszechnioną i bliską większej grupie ludzi niż jest faktycznie
    - Wybieranie tylko szokujących i medialnych informacji tak, żeby propagowały się dalej
    - Przykład: Farmy trolli amplifikujące radykalne przekazy w danym kraju w celu wywoływania powszechnej frustracji i niechęci do rządu
    - [Broszurka o metodach dezinformacji](https://www.cisa.gov/sites/default/files/publications/tactics-of-disinformation_508.pdf)
17. Grassrooting i [astroturfing](https://en.wikipedia.org/wiki/Astroturfing)
    - Grassrooting to sytuacja w której lokalne organizacje wywierają polityczny wpływ na decydentów w celu ulepszenia swojego środowiska
    - Astroturfing to udawanie powyższego, tworzenie fikcyjnych organizacji lub sponsorowanie istniejących aby wywrzeć wrażenie, że oddolne inicjatywy społeczne popierają te lub inne decyzje polityczne
    - Przykład: wsparcie potentata ropy przez "organizacje zajmujące się tematami ekologicznymi"
18. Profilowość
    - Pochodzi z książki [You and Your Profile: Identity After Authenticity](https://cup.columbia.edu/book/you-and-your-profile/9780231196017)
    - Profil w definicji tej książki to precyzyjnie skonstruowana tożsamość, która ma osiągać swój cel (np. sprzedaż poprzez markę osobistą eksperta w internecie, popularność na Instagramie)
    - Profil jest tylko częścią ekspresji człowieka, który "wykonuje" ten profil
    - Pojęcie to jest opisane w relacji do dwóch innych: "autentyczności" (Authenticity) i "szczerości" (Sincerity)
    - Więcej w materiale na [YouTube](https://www.youtube.com/watch?v=r-3gG8cH_oA)
19. Technologia emocji
    - Narzędzia do rozpoznawania i imitowania ludzkich emocji mogą pojawić się dużo wcześniej niż nam się wydaje
    - Już dziś mamy duże postępy i duże pieniądze do zaoszczędzenia w obszarze motion capture. Model, który pozwoli nam przyśpieszyć generowanie bogatej palety zachowań emocjonalnych aktora na podstawie 2 godzin nagrania, można równie dobrze będzie wykorzystać do pokazywania emocji ludziom na ekranie w intuicyjny sposób.
20. Symulakrum
    - Weźmy dowolny symbol i zrekontekstualizujmy go (8). Gdy zrobimy to odpowiednio wiele razy, symbol ten zatraci jakiekolwiek znaczenie i stanie sie oderwanym od rzeczywistości pustym naczyniem nowych znaczeń.
    - Więcej w [notatce Symulakry i Symulacja Jean Boudrillarda](books/symulakry-symulacja.md)
    - Przykład: Memy w internecie
    - Przykład: Rozmycie pewnych pojęć politycznych
21. Zniekształcona rzeczywistość (AI)
    - Część Mapy Trendów
    - Zniekształcona rzeczywistość pojawia się na skutek znaczącego spadku kosztów powstania treści internetowych dzięki narzędziom AI (głos, video, deep fakes, generowany tekst).
    - Na wygenerowanych przez AI treściach ludzie i AI będą się uczyć, chociażby języka angielskiego
    - Efekty tego mogą być rozmaite, ale na pewno w pewien sposób zmienią rzeczywistość taką jaką postrzegamy.

Źródła:

- [Mapa Trendów infuture.institute](https://infuture.institute/mapa-trendow/)
- [Cynefin](https://cynefin.io/)

Zaktualizowano: 2024-05-31

Następna aktualizacja zaplanowana na 2024-06-09